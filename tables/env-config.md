# Environment & Serving Configuration

| Component           | Configuration            |
|--------------------|-------------------------|
| Model               | GPT-OSS-120B            |
| Serving Platform    | KServe (on Kubernetes)  |
| Inference Engine    | vLLM backend            |
| GPU Hardware        | NVIDIA H100 96GB        |
| Test Duration       | 12â€“15 minutes per scenario |
| Streaming Response  | Enabled                 |
